{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13697865,"sourceType":"datasetVersion","datasetId":8713138}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# CELL 1: INSTALL DEPENDENCIES (FIXED VERSION)\n# ============================================================================\nprint(\"ðŸ”§ Installing dependencies...\")\n\n# Install compatible versions to avoid import errors\n!pip install -q langchain==0.1.0\n!pip install -q langchain-community\n!pip install -q chromadb==0.4.22\n\n# Install sentence-transformers with newer version that's compatible with modern huggingface-hub\n!pip install -q sentence-transformers>=2.7.0\n\n!pip install -q pypdf2\n!pip install -q pymupdf\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install -q tqdm\n\nprint(\"âœ… All dependencies installed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:52:05.785152Z","iopub.execute_input":"2025-11-11T21:52:05.785430Z","iopub.status.idle":"2025-11-11T21:52:32.094963Z","shell.execute_reply.started":"2025-11-11T21:52:05.785410Z","shell.execute_reply":"2025-11-11T21:52:32.093929Z"}},"outputs":[{"name":"stdout","text":"ðŸ”§ Installing dependencies...\nâœ… All dependencies installed!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: IMPORTS\n# ============================================================================\nimport os\nimport sys\nimport PyPDF2\nimport fitz  # PyMuPDF for better text extraction\nimport warnings\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport json\n\nwarnings.filterwarnings('ignore')\n\n# LangChain imports\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.schema import Document\n\nprint(\"âœ… All imports successful\")\nprint(f\"ðŸ“… Processing started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:52:32.096472Z","iopub.execute_input":"2025-11-11T21:52:32.096714Z","iopub.status.idle":"2025-11-11T21:52:32.893077Z","shell.execute_reply.started":"2025-11-11T21:52:32.096689Z","shell.execute_reply":"2025-11-11T21:52:32.892431Z"}},"outputs":[{"name":"stdout","text":"âœ… All imports successful\nðŸ“… Processing started: 2025-11-11 21:52:32\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# CELL 3: CONFIGURATION\n# ============================================================================\nclass Config:\n    \"\"\"Configuration for document processing and embedding creation\"\"\"\n    \n    # Dataset path - UPDATE THIS to match your dataset name\n    DATA_PATH = \"/kaggle/input/indian-mining-laws-dataset\"\n    \n    # Output paths\n    VECTORSTORE_PATH = \"/kaggle/working/mining_laws_vectorstore\"\n    PROCESSED_TEXT_PATH = \"/kaggle/working/processed_texts\"\n    METADATA_PATH = \"/kaggle/working/document_metadata.json\"\n    \n    # Embedding configuration\n    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n    # Alternative models (uncomment to use):\n    # EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"  # Better quality, slower\n    # EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\"  # Good balance\n    \n    # Text chunking parameters\n    CHUNK_SIZE = 1000  # Characters per chunk\n    CHUNK_OVERLAP = 200  # Overlap between chunks\n    \n    # Processing options\n    USE_PYMUPDF = True  # Better PDF extraction\n    SAVE_PROCESSED_TEXT = True  # Save extracted text for inspection\n    \n    # Device configuration\n    DEVICE = 'cuda' if os.system('nvidia-smi') == 0 else 'cpu'\n\nconfig = Config()\nprint(f\"âœ… Configuration loaded\")\nprint(f\"ðŸ“Š Device: {config.DEVICE}\")\nprint(f\"ðŸ“ Dataset path: {config.DATA_PATH}\")\n\n# Create output directories\nos.makedirs(config.PROCESSED_TEXT_PATH, exist_ok=True)\nos.makedirs(config.VECTORSTORE_PATH, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:52:32.893858Z","iopub.execute_input":"2025-11-11T21:52:32.894210Z","iopub.status.idle":"2025-11-11T21:52:32.953158Z","shell.execute_reply.started":"2025-11-11T21:52:32.894191Z","shell.execute_reply":"2025-11-11T21:52:32.952517Z"}},"outputs":[{"name":"stdout","text":"Tue Nov 11 21:52:32 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nâœ… Configuration loaded\nðŸ“Š Device: cuda\nðŸ“ Dataset path: /kaggle/input/indian-mining-laws-dataset\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: ADVANCED PDF TEXT EXTRACTOR\n# ============================================================================\nclass PDFTextExtractor:\n    \"\"\"Extract text from PDFs using multiple methods\"\"\"\n    \n    def __init__(self, use_pymupdf=True):\n        self.use_pymupdf = use_pymupdf\n    \n    def extract_with_pymupdf(self, pdf_path: str) -> str:\n        \"\"\"Extract text using PyMuPDF (better quality)\"\"\"\n        try:\n            text = \"\"\n            doc = fitz.open(pdf_path)\n            \n            for page_num in range(len(doc)):\n                page = doc[page_num]\n                text += page.get_text()\n            \n            doc.close()\n            return text.strip()\n        except Exception as e:\n            print(f\"âš ï¸  PyMuPDF failed: {e}\")\n            return \"\"\n    \n    def extract_with_pypdf2(self, pdf_path: str) -> str:\n        \"\"\"Extract text using PyPDF2 (fallback)\"\"\"\n        try:\n            text = \"\"\n            with open(pdf_path, 'rb') as file:\n                pdf_reader = PyPDF2.PdfReader(file)\n                \n                for page in pdf_reader.pages:\n                    try:\n                        text += page.extract_text() + \"\\n\"\n                    except:\n                        continue\n            \n            return text.strip()\n        except Exception as e:\n            print(f\"âš ï¸  PyPDF2 failed: {e}\")\n            return \"\"\n    \n    def extract_text(self, pdf_path: str) -> str:\n        \"\"\"Extract text with fallback methods\"\"\"\n        # Try PyMuPDF first (better quality)\n        if self.use_pymupdf:\n            text = self.extract_with_pymupdf(pdf_path)\n            if text and len(text) > 100:\n                return text\n        \n        # Fallback to PyPDF2\n        text = self.extract_with_pypdf2(pdf_path)\n        return text\n    \n    def clean_text(self, text: str) -> str:\n        \"\"\"Clean extracted text\"\"\"\n        # Remove excessive whitespace\n        text = ' '.join(text.split())\n        \n        # Remove control characters\n        text = ''.join(char for char in text if char.isprintable() or char == '\\n')\n        \n        return text\n\nprint(\"âœ… PDFTextExtractor class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:52:32.954946Z","iopub.execute_input":"2025-11-11T21:52:32.955140Z","iopub.status.idle":"2025-11-11T21:52:32.963421Z","shell.execute_reply.started":"2025-11-11T21:52:32.955125Z","shell.execute_reply":"2025-11-11T21:52:32.962773Z"}},"outputs":[{"name":"stdout","text":"âœ… PDFTextExtractor class defined\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: DOCUMENT PROCESSOR WITH EMBEDDINGS\n# ============================================================================\nclass MiningLawsEmbeddingProcessor:\n    \"\"\"Complete document processing and embedding creation pipeline\"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.pdf_extractor = PDFTextExtractor(use_pymupdf=config.USE_PYMUPDF)\n        \n        # Initialize embeddings model\n        print(f\"ðŸ”„ Loading embedding model: {config.EMBEDDING_MODEL}\")\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=config.EMBEDDING_MODEL,\n            model_kwargs={'device': config.DEVICE},\n            encode_kwargs={'normalize_embeddings': True}\n        )\n        print(\"âœ… Embedding model loaded\")\n        \n        # Initialize text splitter\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=config.CHUNK_SIZE,\n            chunk_overlap=config.CHUNK_OVERLAP,\n            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \": \", \" \", \"\"],\n            length_function=len,\n            is_separator_regex=False\n        )\n        \n        # Storage for metadata\n        self.document_metadata = {}\n        self.processing_stats = {\n            'total_files': 0,\n            'successful': 0,\n            'failed': 0,\n            'total_chunks': 0,\n            'total_chars': 0,\n            'total_words': 0\n        }\n    \n    def classify_document(self, filename: str) -> Dict[str, str]:\n        \"\"\"Classify document type and extract metadata from filename\"\"\"\n        filename_lower = filename.lower()\n        \n        # Document type classification\n        if 'act' in filename_lower and 'report' not in filename_lower:\n            doc_type = 'Act'\n        elif 'regulation' in filename_lower or 'rule' in filename_lower:\n            doc_type = 'Regulation'\n        elif 'circular' in filename_lower:\n            doc_type = 'Circular'\n        elif 'amendment' in filename_lower:\n            doc_type = 'Amendment'\n        elif 'report' in filename_lower:\n            doc_type = 'Report'\n        elif 'policy' in filename_lower:\n            doc_type = 'Policy'\n        elif 'guideline' in filename_lower:\n            doc_type = 'Guideline'\n        elif 'scheme' in filename_lower:\n            doc_type = 'Scheme'\n        elif 'development' in filename_lower:\n            doc_type = 'Development Plan'\n        else:\n            doc_type = 'Document'\n        \n        # Extract year\n        import re\n        year_match = re.search(r'(19|20)\\d{2}', filename)\n        year = year_match.group() if year_match else 'Unknown'\n        \n        # Subject classification\n        subject = self._classify_subject(filename_lower)\n        \n        return {\n            'doc_type': doc_type,\n            'year': year,\n            'subject': subject\n        }\n    \n    def _classify_subject(self, filename: str) -> str:\n        \"\"\"Classify document subject\"\"\"\n        if 'coal' in filename:\n            return 'Coal Mining'\n        elif 'atomic' in filename or 'energy' in filename:\n            return 'Atomic Minerals'\n        elif 'dgms' in filename:\n            return 'Safety & Inspection'\n        elif 'employment' in filename or 'women' in filename:\n            return 'Labour & Employment'\n        elif 'pension' in filename or 'provident' in filename:\n            return 'Welfare & Benefits'\n        elif 'electricity' in filename:\n            return 'Electricity & Energy'\n        elif 'mineral' in filename and 'development' in filename:\n            return 'Mineral Development'\n        elif 'vocational' in filename or 'training' in filename:\n            return 'Training & Education'\n        elif 'court' in filename or 'enquiry' in filename:\n            return 'Legal & Judicial'\n        else:\n            return 'General Mining'\n    \n    def process_single_document(self, pdf_path: str) -> Tuple[List[Document], Dict]:\n        \"\"\"Process a single PDF document\"\"\"\n        filename = os.path.basename(pdf_path)\n        \n        # Extract text\n        text = self.pdf_extractor.extract_text(pdf_path)\n        \n        if not text or len(text) < 100:\n            return [], {'error': 'Insufficient text extracted'}\n        \n        # Clean text\n        text = self.pdf_extractor.clean_text(text)\n        \n        # Get document classification\n        classification = self.classify_document(filename)\n        \n        # Create base metadata\n        base_metadata = {\n            'source': filename,\n            'path': pdf_path,\n            'doc_type': classification['doc_type'],\n            'year': classification['year'],\n            'subject': classification['subject'],\n            'char_count': len(text),\n            'word_count': len(text.split()),\n            'processed_date': datetime.now().isoformat()\n        }\n        \n        # Save processed text if configured\n        if self.config.SAVE_PROCESSED_TEXT:\n            text_file = os.path.join(\n                self.config.PROCESSED_TEXT_PATH,\n                filename.replace('.pdf', '.txt')\n            )\n            with open(text_file, 'w', encoding='utf-8') as f:\n                f.write(text)\n        \n        # Split into chunks\n        chunks = self.text_splitter.split_text(text)\n        \n        # Create Document objects with metadata\n        documents = []\n        for idx, chunk in enumerate(chunks):\n            chunk_metadata = base_metadata.copy()\n            chunk_metadata.update({\n                'chunk_id': idx,\n                'chunk_total': len(chunks),\n                'chunk_chars': len(chunk)\n            })\n            documents.append(Document(page_content=chunk, metadata=chunk_metadata))\n        \n        # Update stats\n        self.processing_stats['total_chunks'] += len(chunks)\n        self.processing_stats['total_chars'] += len(text)\n        self.processing_stats['total_words'] += len(text.split())\n        \n        return documents, base_metadata\n    \n    def process_all_documents(self) -> List[Document]:\n        \"\"\"Process all documents in the dataset\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ðŸš€ STARTING DOCUMENT PROCESSING & EMBEDDING CREATION\")\n        print(\"=\"*80)\n        \n        all_documents = []\n        \n        # Get all PDF files\n        pdf_files = []\n        for root, dirs, files in os.walk(self.config.DATA_PATH):\n            for file in files:\n                if file.endswith('.pdf'):\n                    pdf_files.append(os.path.join(root, file))\n        \n        self.processing_stats['total_files'] = len(pdf_files)\n        \n        print(f\"\\nðŸ“Š Found {len(pdf_files)} PDF files\")\n        print(f\"ðŸ“ Processing from: {self.config.DATA_PATH}\")\n        print(f\"âš™ï¸  Chunk size: {self.config.CHUNK_SIZE}, Overlap: {self.config.CHUNK_OVERLAP}\")\n        print(f\"ðŸ”¤ Embedding model: {self.config.EMBEDDING_MODEL}\\n\")\n        \n        # Process each file with progress bar\n        for pdf_path in tqdm(pdf_files, desc=\"Processing documents\"):\n            filename = os.path.basename(pdf_path)\n            \n            try:\n                documents, metadata = self.process_single_document(pdf_path)\n                \n                if documents:\n                    all_documents.extend(documents)\n                    self.document_metadata[filename] = metadata\n                    self.processing_stats['successful'] += 1\n                    tqdm.write(f\"âœ… {filename}: {len(documents)} chunks\")\n                else:\n                    self.processing_stats['failed'] += 1\n                    tqdm.write(f\"âŒ {filename}: Failed to extract text\")\n                    \n            except Exception as e:\n                self.processing_stats['failed'] += 1\n                tqdm.write(f\"âŒ {filename}: Error - {str(e)}\")\n        \n        # Print summary\n        self._print_processing_summary()\n        \n        # Save metadata\n        self._save_metadata()\n        \n        return all_documents\n    \n    def _print_processing_summary(self):\n        \"\"\"Print processing statistics\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ðŸ“Š PROCESSING SUMMARY\")\n        print(\"=\"*80)\n        print(f\"Total Files Found:      {self.processing_stats['total_files']}\")\n        print(f\"âœ… Successfully Processed: {self.processing_stats['successful']}\")\n        print(f\"âŒ Failed:                 {self.processing_stats['failed']}\")\n        print(f\"ðŸ“¦ Total Chunks Created:   {self.processing_stats['total_chunks']:,}\")\n        print(f\"ðŸ“ Total Characters:       {self.processing_stats['total_chars']:,}\")\n        print(f\"ðŸ”¤ Total Words:            {self.processing_stats['total_words']:,}\")\n        print(f\"ðŸ“Š Avg Chunks/Document:    {self.processing_stats['total_chunks'] / max(self.processing_stats['successful'], 1):.1f}\")\n        print(\"=\"*80 + \"\\n\")\n    \n    def _save_metadata(self):\n        \"\"\"Save document metadata to JSON\"\"\"\n        metadata_file = self.config.METADATA_PATH\n        \n        save_data = {\n            'processing_stats': self.processing_stats,\n            'documents': self.document_metadata,\n            'config': {\n                'embedding_model': self.config.EMBEDDING_MODEL,\n                'chunk_size': self.config.CHUNK_SIZE,\n                'chunk_overlap': self.config.CHUNK_OVERLAP\n            }\n        }\n        \n        with open(metadata_file, 'w') as f:\n            json.dump(save_data, f, indent=2)\n        \n        print(f\"ðŸ’¾ Metadata saved to: {metadata_file}\")\n    \n    def create_embeddings_and_vectorstore(self, documents: List[Document]) -> Chroma:\n        \"\"\"Create embeddings and store in Chroma vector database\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ðŸ”„ CREATING EMBEDDINGS & VECTOR STORE\")\n        print(\"=\"*80)\n        print(f\"ðŸ“¦ Total document chunks: {len(documents)}\")\n        print(f\"ðŸ”¤ Embedding model: {self.config.EMBEDDING_MODEL}\")\n        print(f\"ðŸ’¾ Vector store path: {self.config.VECTORSTORE_PATH}\")\n        print(\"\\nâ³ This may take several minutes...\\n\")\n        \n        # Create vector store with progress tracking\n        vectorstore = Chroma.from_documents(\n            documents=documents,\n            embedding=self.embeddings,\n            persist_directory=self.config.VECTORSTORE_PATH,\n            collection_name=\"mining_laws\"\n        )\n        \n        print(\"\\nâœ… Vector store created successfully!\")\n        print(f\"ðŸ“Š Total embeddings: {len(documents)}\")\n        print(f\"ðŸ’¾ Saved at: {self.config.VECTORSTORE_PATH}\")\n        print(\"=\"*80 + \"\\n\")\n        \n        return vectorstore\n    \n    def load_existing_vectorstore(self) -> Chroma:\n        \"\"\"Load previously created vector store\"\"\"\n        print(\"\\nðŸ“‚ Loading existing vector store...\")\n        \n        if not os.path.exists(self.config.VECTORSTORE_PATH):\n            raise FileNotFoundError(\n                f\"Vector store not found at {self.config.VECTORSTORE_PATH}. \"\n                \"Please run document processing first.\"\n            )\n        \n        vectorstore = Chroma(\n            persist_directory=self.config.VECTORSTORE_PATH,\n            embedding_function=self.embeddings,\n            collection_name=\"mining_laws\"\n        )\n        \n        print(\"âœ… Vector store loaded successfully\\n\")\n        return vectorstore\n\nprint(\"âœ… MiningLawsEmbeddingProcessor class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:52:32.964168Z","iopub.execute_input":"2025-11-11T21:52:32.964406Z","iopub.status.idle":"2025-11-11T21:52:32.990932Z","shell.execute_reply.started":"2025-11-11T21:52:32.964381Z","shell.execute_reply":"2025-11-11T21:52:32.990320Z"}},"outputs":[{"name":"stdout","text":"âœ… MiningLawsEmbeddingProcessor class defined\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: PROCESS DOCUMENTS & CREATE EMBEDDINGS (MAIN EXECUTION)\n# ============================================================================\nprint(\"\\nðŸŽ¯ Initializing processor...\")\n\n# Create processor instance\nprocessor = MiningLawsEmbeddingProcessor(config)\n\n# Process all documents\nprint(\"\\nâš¡ Starting document processing pipeline...\")\ndocuments = processor.process_all_documents()\n\n# Create embeddings and vector store\nvectorstore = processor.create_embeddings_and_vectorstore(documents)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ðŸŽ‰ EMBEDDING CREATION COMPLETE!\")\nprint(\"=\"*80)\nprint(f\"âœ… {len(documents)} document chunks embedded\")\nprint(f\"ðŸ’¾ Vector store saved at: {config.VECTORSTORE_PATH}\")\nprint(f\"ðŸ“„ Processed texts saved at: {config.PROCESSED_TEXT_PATH}\")\nprint(f\"ðŸ“Š Metadata saved at: {config.METADATA_PATH}\")\nprint(\"=\"*80 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:52:32.991756Z","iopub.execute_input":"2025-11-11T21:52:32.992028Z","iopub.status.idle":"2025-11-11T21:53:59.308031Z","shell.execute_reply.started":"2025-11-11T21:52:32.992005Z","shell.execute_reply":"2025-11-11T21:53:59.307239Z"}},"outputs":[{"name":"stdout","text":"\nðŸŽ¯ Initializing processor...\nðŸ”„ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26bf909b77134673890f50f3f7aac802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce617efa2f174b95b531ecbc11e757e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44aae51749e44aaa9606dee4c7dc6392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec5e580461f4068aa815eb1e27ddf86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81dc26938b5a4d3a81d62aabf1f74d3d"}},"metadata":{}},{"name":"stderr","text":"2025-11-11 21:52:49.177593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762897969.354563     237 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762897969.402532     237 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52db0a665c04a199755cd762f90060e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267261b1c95b427faf66694c084eea80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2689aa24e0c7446fa7c6c9f1a3acefa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3473ad8a76a443798217510b6cdb0f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07a77c6ff4f64b0eb362c62df822e0d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1335cc91192e494aa8c592cca315c0b6"}},"metadata":{}},{"name":"stdout","text":"âœ… Embedding model loaded\n\nâš¡ Starting document processing pipeline...\n\n================================================================================\nðŸš€ STARTING DOCUMENT PROCESSING & EMBEDDING CREATION\n================================================================================\n\nðŸ“Š Found 40 PDF files\nðŸ“ Processing from: /kaggle/input/indian-mining-laws-dataset\nâš™ï¸  Chunk size: 1000, Overlap: 200\nðŸ”¤ Embedding model: sentence-transformers/all-MiniLM-L6-v2\n\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   2%|â–Ž         | 1/40 [00:00<00:14,  2.71it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… THE CENTRAL ELECTRICITY 2023.pdf: 403 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   5%|â–Œ         | 2/40 [00:00<00:14,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Rajmahal Court of Enquiry Report.pdf: 457 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  10%|â–ˆ         | 4/40 [00:01<00:08,  4.04it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… THE FACTORIES ACT 1948.pdf: 274 chunks\nâœ… THE MINES ACT 1952.pdf: 190 chunks\nâœ… ATOMIC ENERGY (WORKING OF THE MINES MINERALS AND HANDLING OF PRESCRIBED SUBSTRANCES) RULES 1984.pdf: 46 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  18%|â–ˆâ–Š        | 7/40 [00:01<00:04,  8.09it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… THE MINES RESCUE RULES 1985.pdf: 43 chunks\nâœ… List of DGMS Approved Closed Circuit Self Contained Breathing Apparatus.pdf: 21 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:01<00:03,  8.83it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… The_oil_mines_regulation_2017.pdf: 262 chunks\nâœ… THE WEST BENGAL FACTORIES AND MINES( CONTROL OF DISMANTLING) ACT 1948.pdf: 8 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:01<00:03,  8.28it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Review of the Approval policy-2015 (Second Revision-2016).pdf: 144 chunks\nâœ… Employment Of Women In Mines.pdf: 5 chunks\nâœ… Mines and Mineral Development Restoration and Rehabilitation Fund.pdf: 20 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:02<00:04,  6.30it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… THE EXPLOSIVES RULES 2008.pdf: 726 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:04<00:16,  1.57it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-13.pdf: 1512 chunks\nâœ… THE COAL MINES LABOUR WELFARE FUND (REPEAL) ACT 1986.pdf: 10 chunks\nâœ… THE MAHARASHTRA ABOLITION OF SUBSISTING PROPRIETARY 1985.pdf: 19 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:05<00:11,  2.08it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-14.pdf: 375 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:07<00:16,  1.37it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… SANKET NON-COAL MINES 2016-22.pdf: 1096 chunks\nMuPDF error: format error: non-page object in page tree\n\nMuPDF error: syntax error: cannot find ExtGState resource 'R9'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ExtGState resource 'R14'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\nMuPDF error: syntax error: cannot find ColorSpace resource 'R11'\n\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:08<00:08,  2.13it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-10.pdf: 188 chunks\nâœ… THE COAL MINES PROVIDENT FUND AND MISCELLANEOUS PROVISION ACT 1948.pdf: 69 chunks\nâœ… THE MINES CRECHE RULES 1966.pdf: 22 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:08<00:07,  2.48it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… COAL MINES PENSION SCHEME 1998.pdf: 128 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:09<00:06,  2.49it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… COAL MINES PROVIDENT FUND SCHEME 1948.pdf: 367 chunks\nâœ… THE COAL MINES( CONSERVATION AND DEVELOPMENT) AMENDMENT RULES 2011.pdf: 45 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:10<00:06,  2.17it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-08.pdf: 383 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:10<00:05,  2.38it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… THE METALLIFEROUS MINES REGULATIONS 1961.pdf: 528 chunks\nâœ… THE MINES RULES 1955.pdf: 147 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:11<00:04,  2.41it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-11.pdf: 206 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:12<00:06,  1.67it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-09.pdf: 669 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:13<00:05,  1.79it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-06.pdf: 202 chunks\nâœ… THE MATERNITY BENEFIT (MINES AND CIRCUS ) RULES 1963.pdf: 35 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:14<00:04,  1.66it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… SANKET COAL MINES 2016-22.pdf: 984 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:15<00:02,  2.48it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-12.pdf: 205 chunks\nâœ… Amendment in the Mines Rules 1955.pdf: 4 chunks\nâœ… THE COAL MINES (SPECIAL PROVISIONS) ACT 2015.pdf: 109 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:15<00:01,  3.00it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… MINES VOCATIONAL TRAINING RULES 1966.pdf: 122 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:15<00:00,  3.16it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… The Mines and Minerals (Development and Regulation) 1957.pdf: 250 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:16<00:00,  2.02it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… DGMS_Annual_Report_2014_Eng-05.pdf: 189 chunks\nâœ… The Bihar Land Reforms Laws(Regulating Mines and Minerals) Validation Act 1969.pdf: 7 chunks\n","output_type":"stream"},{"name":"stderr","text":"Processing documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:17<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… COAL MINES REGULATION 2017.pdf: 639 chunks\n\n================================================================================\nðŸ“Š PROCESSING SUMMARY\n================================================================================\nTotal Files Found:      40\nâœ… Successfully Processed: 40\nâŒ Failed:                 0\nðŸ“¦ Total Chunks Created:   11,109\nðŸ“ Total Characters:       8,134,814\nðŸ”¤ Total Words:            1,412,306\nðŸ“Š Avg Chunks/Document:    277.7\n================================================================================\n\nðŸ’¾ Metadata saved to: /kaggle/working/document_metadata.json\n\n================================================================================\nðŸ”„ CREATING EMBEDDINGS & VECTOR STORE\n================================================================================\nðŸ“¦ Total document chunks: 11109\nðŸ”¤ Embedding model: sentence-transformers/all-MiniLM-L6-v2\nðŸ’¾ Vector store path: /kaggle/working/mining_laws_vectorstore\n\nâ³ This may take several minutes...\n\n","output_type":"stream"},{"name":"stderr","text":"ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\nERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Vector store created successfully!\nðŸ“Š Total embeddings: 11109\nðŸ’¾ Saved at: /kaggle/working/mining_laws_vectorstore\n================================================================================\n\n\n================================================================================\nðŸŽ‰ EMBEDDING CREATION COMPLETE!\n================================================================================\nâœ… 11109 document chunks embedded\nðŸ’¾ Vector store saved at: /kaggle/working/mining_laws_vectorstore\nðŸ“„ Processed texts saved at: /kaggle/working/processed_texts\nðŸ“Š Metadata saved at: /kaggle/working/document_metadata.json\n================================================================================\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: TEST EMBEDDINGS & SIMILARITY SEARCH\n# ============================================================================\nprint(\"ðŸ§ª TESTING EMBEDDINGS WITH SIMILARITY SEARCH\")\nprint(\"=\"*80 + \"\\n\")\n\ndef test_similarity_search(query: str, k: int = 3):\n    \"\"\"Test similarity search with embeddings\"\"\"\n    print(f\"ðŸ” Query: '{query}'\")\n    print(f\"ðŸ“Š Retrieving top {k} similar chunks...\\n\")\n    \n    results = vectorstore.similarity_search(query, k=k)\n    \n    for idx, doc in enumerate(results, 1):\n        print(f\"[{idx}] Source: {doc.metadata['source']}\")\n        print(f\"    Type: {doc.metadata['doc_type']} | Subject: {doc.metadata['subject']}\")\n        print(f\"    Year: {doc.metadata['year']}\")\n        print(f\"    Chunk: {doc.metadata['chunk_id']}/{doc.metadata['chunk_total']}\")\n        print(f\"    Preview: {doc.page_content[:200]}...\")\n        print(f\"    Relevance: High\\n\")\n\n# Test queries\ntest_queries = [\n    \"What are the safety requirements for coal mines?\",\n    \"minimum age for workers in mines\",\n    \"DGMS annual report findings\",\n    \"employment of women in mining\",\n]\n\nfor query in test_queries:\n    test_similarity_search(query, k=3)\n    print(\"-\" * 80 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:53:59.308849Z","iopub.execute_input":"2025-11-11T21:53:59.309493Z","iopub.status.idle":"2025-11-11T21:53:59.361559Z","shell.execute_reply.started":"2025-11-11T21:53:59.309471Z","shell.execute_reply":"2025-11-11T21:53:59.360763Z"}},"outputs":[{"name":"stderr","text":"ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª TESTING EMBEDDINGS WITH SIMILARITY SEARCH\n================================================================================\n\nðŸ” Query: 'What are the safety requirements for coal mines?'\nðŸ“Š Retrieving top 3 similar chunks...\n\n[1] Source: DGMS_Annual_Report_2014_Eng-09.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 415/669\n    Preview: . Provisions of the Coal Mines Regulations, 1957, relating to mine working; explosives and shortfiring; haulage; ventilation; precautions against danger from fire, dust, gas and water and of other pro...\n    Relevance: High\n\n[2] Source: DGMS_Annual_Report_2014_Eng-09.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 640/669\n    Preview: . Provisions of the Coal Mines Regulations, 1957, relating to mine working; explosives and shortfiring; haulage; ventilation; precautions against danger from fire, dust, gas and water and of other pro...\n    Relevance: High\n\n[3] Source: DGMS_Annual_Report_2014_Eng-13.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 1365/1512\n    Preview: : The management of the mines should conduct a risk assessment process to identify and hazard that could influence the safety and health of workers while installing and maintenance of coal/ore handlin...\n    Relevance: High\n\n--------------------------------------------------------------------------------\n\nðŸ” Query: 'minimum age for workers in mines'\nðŸ“Š Retrieving top 3 similar chunks...\n\n[1] Source: SANKET NON-COAL MINES 2016-22.pdf\n    Type: Document | Subject: Coal Mining\n    Year: 2016\n    Chunk: 458/1096\n    Preview: eighteen years of age not allowed to work in mine, as required under the provisions of Regulation 106(2)(b), Regulation 106(1)(a), Regulation 106(3), Regulation 106(5) of the Metalliferous Mines Regul...\n    Relevance: High\n\n[2] Source: THE MINES ACT 1952.pdf\n    Type: Act | Subject: General Mining\n    Year: 1952\n    Chunk: 100/190\n    Preview: . (2)Every certificate granted by a certifying surgeon on a reference under sub-section (1), shall, for the purpose of this Act, be conclusive evidence of the matters referred therein.] 44.[Working ho...\n    Relevance: High\n\n[3] Source: THE MINES ACT 1952.pdf\n    Type: Act | Subject: General Mining\n    Year: 1952\n    Chunk: 97/190\n    Preview: . Employment of persons below eighteen years of age.â€”(1)After the commencement of the the Mines (Amendment) Act, 1983 (42 of 1983), no person below eighteen years of age shall be allowed to work in an...\n    Relevance: High\n\n--------------------------------------------------------------------------------\n\nðŸ” Query: 'DGMS annual report findings'\nðŸ“Š Retrieving top 3 similar chunks...\n\n[1] Source: DGMS_Annual_Report_2014_Eng-12.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 25/205\n    Preview: . DGMS Annual Report, 2012 11 TABLE...\n    Relevance: High\n\n[2] Source: DGMS_Annual_Report_2014_Eng-06.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 26/202\n    Preview: . DGMS Annual Report,2006 11 TABLE...\n    Relevance: High\n\n[3] Source: DGMS_Annual_Report_2014_Eng-11.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 25/206\n    Preview: . DGMS Annual Report, 2011 10 TABLE...\n    Relevance: High\n\n--------------------------------------------------------------------------------\n\nðŸ” Query: 'employment of women in mining'\nðŸ“Š Retrieving top 3 similar chunks...\n\n[1] Source: DGMS_Annual_Report_2014_Eng-09.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 363/669\n    Preview: . Personal Management and Organizational behavior Selection; training and development of human resources for mining enterprises; leadership; study of traditional leader behaviour; autocratic; democrat...\n    Relevance: High\n\n[2] Source: DGMS_Annual_Report_2014_Eng-09.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 274/669\n    Preview: . Economic Impact of Mining Economics of mining effect on community â€“ before, during and after mining. Materials Management for mining sector. Behavioural Sciences for Management :- Conflict managemen...\n    Relevance: High\n\n[3] Source: DGMS_Annual_Report_2014_Eng-09.pdf\n    Type: Report | Subject: Safety & Inspection\n    Year: 2014\n    Chunk: 552/669\n    Preview: . Economic Impact of Mining Economics of mining effect on community â€“ before, during and after mining. Materials Management for mining sector. Behavioural Sciences for Management :- Conflict managemen...\n    Relevance: High\n\n--------------------------------------------------------------------------------\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: EMBEDDING STATISTICS & ANALYSIS\n# ============================================================================\ndef get_embedding_statistics(documents):\n    \"\"\"Get detailed statistics about embeddings\"\"\"\n    \n    stats = {\n        'by_doc_type': {},\n        'by_subject': {},\n        'by_year': {},\n        'chunk_size_distribution': {\n            'min': float('inf'),\n            'max': 0,\n            'avg': 0\n        }\n    }\n    \n    total_chunks = 0\n    \n    for doc in documents:\n        # By document type\n        doc_type = doc.metadata['doc_type']\n        stats['by_doc_type'][doc_type] = stats['by_doc_type'].get(doc_type, 0) + 1\n        \n        # By subject\n        subject = doc.metadata['subject']\n        stats['by_subject'][subject] = stats['by_subject'].get(subject, 0) + 1\n        \n        # By year\n        year = doc.metadata['year']\n        stats['by_year'][year] = stats['by_year'].get(year, 0) + 1\n        \n        # Chunk size\n        chunk_size = doc.metadata['chunk_chars']\n        stats['chunk_size_distribution']['min'] = min(stats['chunk_size_distribution']['min'], chunk_size)\n        stats['chunk_size_distribution']['max'] = max(stats['chunk_size_distribution']['max'], chunk_size)\n        total_chunks += 1\n    \n    stats['chunk_size_distribution']['avg'] = sum(d.metadata['chunk_chars'] for d in documents) / len(documents)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ðŸ“Š EMBEDDING STATISTICS\")\n    print(\"=\"*80)\n    \n    print(\"\\nðŸ“‘ By Document Type:\")\n    for dtype, count in sorted(stats['by_doc_type'].items(), key=lambda x: x[1], reverse=True):\n        print(f\"  {dtype:20s}: {count:4d} chunks\")\n    \n    print(\"\\nðŸ”– By Subject:\")\n    for subject, count in sorted(stats['by_subject'].items(), key=lambda x: x[1], reverse=True):\n        print(f\"  {subject:25s}: {count:4d} chunks\")\n    \n    print(\"\\nðŸ“… By Year:\")\n    for year, count in sorted(stats['by_year'].items()):\n        print(f\"  {year}: {count:4d} chunks\")\n    \n    print(\"\\nðŸ“ Chunk Size Distribution:\")\n    print(f\"  Minimum: {stats['chunk_size_distribution']['min']} chars\")\n    print(f\"  Maximum: {stats['chunk_size_distribution']['max']} chars\")\n    print(f\"  Average: {stats['chunk_size_distribution']['avg']:.0f} chars\")\n    \n    print(\"=\"*80 + \"\\n\")\n    \n    return stats\n\n# Get and display statistics\nembedding_stats = get_embedding_statistics(documents)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:53:59.362469Z","iopub.execute_input":"2025-11-11T21:53:59.362774Z","iopub.status.idle":"2025-11-11T21:53:59.384592Z","shell.execute_reply.started":"2025-11-11T21:53:59.362746Z","shell.execute_reply":"2025-11-11T21:53:59.383956Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nðŸ“Š EMBEDDING STATISTICS\n================================================================================\n\nðŸ“‘ By Document Type:\n  Report              : 4386 chunks\n  Regulation          : 2869 chunks\n  Document            : 2528 chunks\n  Act                 :  667 chunks\n  Scheme              :  495 chunks\n  Policy              :  144 chunks\n  Development Plan    :   20 chunks\n\nðŸ”– By Subject:\n  Safety & Inspection      : 3950 chunks\n  Coal Mining              : 3447 chunks\n  General Mining           : 2409 chunks\n  Legal & Judicial         :  457 chunks\n  Electricity & Energy     :  403 chunks\n  Mineral Development      :  270 chunks\n  Training & Education     :  122 chunks\n  Atomic Minerals          :   46 chunks\n  Labour & Employment      :    5 chunks\n\nðŸ“… By Year:\n  1948:  718 chunks\n  1952:  190 chunks\n  1955:  151 chunks\n  1957:  250 chunks\n  1961:  528 chunks\n  1963:   35 chunks\n  1966:  144 chunks\n  1969:    7 chunks\n  1984:   46 chunks\n  1985:   62 chunks\n  1986:   10 chunks\n  1998:  128 chunks\n  2008:  726 chunks\n  2011:   45 chunks\n  2014: 3929 chunks\n  2015:  253 chunks\n  2016: 2080 chunks\n  2017:  901 chunks\n  2023:  403 chunks\n  Unknown:  503 chunks\n\nðŸ“ Chunk Size Distribution:\n  Minimum: 3 chars\n  Maximum: 1000 chars\n  Average: 807 chars\n================================================================================\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: SAVE EMBEDDING INFO FOR LLM INTEGRATION\n# ============================================================================\nembedding_info = {\n    'vectorstore_path': config.VECTORSTORE_PATH,\n    'embedding_model': config.EMBEDDING_MODEL,\n    'total_documents': len(documents),\n    'chunk_size': config.CHUNK_SIZE,\n    'chunk_overlap': config.CHUNK_OVERLAP,\n    'device': config.DEVICE,\n    'created_date': datetime.now().isoformat(),\n    'ready_for_llm': True,\n    'statistics': {\n        'total_chunks': len(documents),\n        'total_files_processed': processor.processing_stats['successful'],\n        'by_doc_type': embedding_stats['by_doc_type'],\n        'by_subject': embedding_stats['by_subject']\n    }\n}\n\nwith open('/kaggle/working/embedding_info.json', 'w') as f:\n    json.dump(embedding_info, f, indent=2)\n\nprint(\"âœ… Embedding info saved for LLM integration\")\nprint(f\"ðŸ“„ File: /kaggle/working/embedding_info.json\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… EMBEDDINGS ARE READY TO BE FED TO LLM!\")\nprint(\"=\"*80)\nprint(\"\\nðŸ“‹ Next Steps:\")\nprint(\"1. âœ… Document processing complete\")\nprint(\"2. âœ… Embeddings created and stored\")\nprint(\"3. âœ… Vector store persisted to disk\")\nprint(\"4. â­ï¸  Ready for LLM integration (RAG pipeline)\")\nprint(\"\\nðŸ’¡ You can now use these embeddings to:\")\nprint(\"   - Retrieve relevant context for user queries\")\nprint(\"   - Feed to LLM for question answering\")\nprint(\"   - Build a complete RAG system\")\nprint(\"=\"*80 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:53:59.385404Z","iopub.execute_input":"2025-11-11T21:53:59.385822Z","iopub.status.idle":"2025-11-11T21:53:59.402993Z","shell.execute_reply.started":"2025-11-11T21:53:59.385777Z","shell.execute_reply":"2025-11-11T21:53:59.402372Z"}},"outputs":[{"name":"stdout","text":"âœ… Embedding info saved for LLM integration\nðŸ“„ File: /kaggle/working/embedding_info.json\n\n================================================================================\nâœ… EMBEDDINGS ARE READY TO BE FED TO LLM!\n================================================================================\n\nðŸ“‹ Next Steps:\n1. âœ… Document processing complete\n2. âœ… Embeddings created and stored\n3. âœ… Vector store persisted to disk\n4. â­ï¸  Ready for LLM integration (RAG pipeline)\n\nðŸ’¡ You can now use these embeddings to:\n   - Retrieve relevant context for user queries\n   - Feed to LLM for question answering\n   - Build a complete RAG system\n================================================================================\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}